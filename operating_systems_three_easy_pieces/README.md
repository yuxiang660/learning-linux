# 配置环境
## 安装“XV6”系统
* Ubuntu下安装qemu
   > sudo apt update && sudo apt-get install qemu-system
   * 安装完成后需要支持`qemu-system-i386`或者`qemu-system-x86_64`命令，否则会出现`Error: Couldn't find a working QEMU executable.`错误。可自行搜索相关依赖
* 安装xv6
   > git clone git://github.com/mit-pdos/xv6-public.git
   > make
* 启动xv6
   > make qemu
   * 在xv6目录下输入上面的命令，即可出现如下xv6窗口：
   ![xv6](./pictures/xv6.png)

# 第2章 操作系统介绍
## 关键问题
* 如何将资源虚拟化？
   * 操作系统通过哪些机制和策略来实现虚拟化？
   * 操作系统如何有效地实现虚拟化？
   * 需要哪些硬件支持？

操作系统将物理资源(如处理器、内存或磁盘)转换为更通用、更强大且更易于使用的虚拟形式。

## 虚拟化CPU
* 所谓的虚拟化CPU，即系统将单个CPU(或其中一小部分)转换为看似无限数量的CPU
* [例子"virtualizing_cpu"](./code/virtualizing_cpu)可以在一个CPU上同时运行多个程序

## 虚拟化内存
* 每个进程访问自己的私有虚拟地址空间，操作系统以某种方式映射到机器的物理内存上
* [例子"virtualizing_memory"](./code/virtualizing_memory)可以在同一个地址(虚拟地址)上读写数据
   ```bash
   > make run
   (480896) memory address of static_var: 0x555555558010
   (480896) memory address of p: 0x5555555596b0

   (480897) memory address of static_var: 0x555555558010
   (480897) memory address of p: 0x5555555596b0
   ```

## 并发
### 关键问题
* 如何构建正确的并发程序？
   * 如果同一个内存空间中有很多并发执行的线程，如何构建一个正确的工作程序？
   * 操作系统需要什么原语？
   * 硬件应该提供哪些机制？
   * 我们如何利用它们来解决并发问题？

并发程序有可能会导致一些深刻而有趣的问题，如：
* [例子"concurrency_thread"](./code/concurrency_thread)展示了并发运行时有可能出现的意外(计数器的值不是期望值)

## 持久性
操作系统通过管理磁盘的软件(文件系统file system)来持久地为用户保存数据。不像操作系统为CPU和内存提供的抽象，操作系统不会为每个应用程序创建专用的虚拟磁盘。相反，它假设用户经常需要共享文件中的信息。
### 关键问题
* 如何持久地存储数据？
   * 持久性需要哪些技术才能正确地实现？
   * 需要哪些机制和策略才能高性能地实现
   * 面对硬件和软件故障，可靠性如何实现？

* [例子"persistence_io"](./code/persistence_io)通过系统调用向磁盘中记录了`hello`信息。

## 设计目标
* 操作系统能做什么
   * 取得CPU、内存和磁盘等物理资源，并对它们进行虚拟化
   * 处理与并发有关的麻烦且棘手的问题
   * 持久地存储文件，从而使它们长期安全
* 设计操作系统的要求
   * 提供高性能
   * 在应用程序之间以及在OS和应用程序之间提供保护
   * 高度可靠，可不间断运行
   * 安全性、移动性

# 第4章 抽象：进程
## 关键问题
* 如何提供有许多CPU的假象？
   * 时分共享
      * CPU在时间上划分给多个任务
   * 空分共享
      * 磁盘在空间上划分给多个任务

* 操作系统以什么策略做出调度决定？
   * 历史信息
      * 哪个程序在最后一分钟要运行更多？
   * 工作负载
      * 运行什么类型的程序？
   * 性能指标
      * 系统是否针对交互式性能或吞吐量进行优化？

* 什么是操作系统的策略和机制分离？
   * 一个通用的设计范式是将高级策略与低级机制分开
   * 机制是解决how问题
      * 例如，操作系统如何执行上下文切换
   * 策略是解决which问题
      * 例如操作系统现在应该运行哪个进程
   * 将策略和机制分开可以轻松针对某个策略改变策略，而不必重新考虑机制

## 什么是进程？
操作系统为正在运行的程序提供的抽象，就是进程。进程的机器状态有下面几个部分组成：
* 内存
   * 指令
   * 数据
* 寄存器
   * PC寄存器
      * 告诉我们程序当前正在执行哪个指令
   * 栈指针stack pointer 和帧指针frame pointer
      * 用于管理函数参数栈、局部变量和返回地址
* 持久存储设备
   * 当前打开的文件列表

## 进程状态
* 运行running
   * 进程正在处理器上执行指令
* 就绪ready
   * 进程已准备好运行，但由于某种原因，操作系统选择不在此时运行
* 阻塞blocked
   * 进程执行了某种操作，直到发生其他事件时才会准备运行

![process_state_transitions](./pictures/process_state_transitions.png)
![process_status](./pictures/process_status.png)

上表中，Process0发起I/O并被阻塞，等待I/O完成。例如，当从磁盘读取数据或等待网络数据包时，进程会被阻塞。OS发现Process0不使用CPU，则开始运行Process1。当Process1运行时，I/O完成，将Process0移回就绪状态。最后，Process1结束，Process0运行，然后完成。OS在上面步骤中做了以下两个决策：
* 决定在Process0发出I/O时，运行Process1
* 当I/O完成时，系统决定不切回Process0

## 进程数据结构
操作系统有一些关键的数据结构来跟踪进程的状态，称为进程列表(process list)或者进程控制块(Process Control Block, PCB)。下面是xv6内核中每个进程的信息：
```c
// the registers xv6 will save and restore
// to stop and subsequently restart a process
struct context
{
   int eip;
   int esp;
   int ebx;
   int ecx;
   int edx;
   int esi;
   int edi;
   int ebp;
};
// the different states a process can be in
enum proc_state
{
   UNUSED,
   EMBRYO,
   SLEEPING,
   RUNNABLE,
   RUNNING,
   ZOMBIE
};
// the information xv6 tracks about each process
// including its register context and state
struct proc
{
   char *mem;    // Start of process memory
   uint sz;      // Size of process memory
   char *kstack; // Bottom of kernel stack for this process
   enum proc_state state;      // Process state
   int pid;                    // Process ID
   struct proc *parent;        // Parent process
   void *chan;                 // If non-zero, sleeping on chan
   int killed;                 // If non-zero, have been killed
   struct file *ofile[NOFILE]; // Open files
   struct inode *cwd;          // Current directory
   struct context context;     // Switch here to run process
   struct trapframe *tf;       // Trap frame for the current interrupt
};
```

# 第5章 插叙：进程API
## 关键问题
* 为什么设计`fork`和`exec`这种奇怪的接口来完成简单的、创建新进程的任务？
   * 分离`fork`和`exec`的做法在构建UNIX shell的时候非常有用，因为这给了shell在fork之后exec之前运行代码的机会，这些代码可以在运行新程序前改变环境，从而让一系列有趣的功能很容易实现。比如，`wc p3.c > newfile.txt` 重定向很容易实现，shell在调用`exec`之前先关闭标准输出，打开文件newfile.txt即可。
   * 参考例子[redirect](./code/redirect)

# 第6章：受限直接执行
## 关键问题
* 如何高效、可控地虚拟化CPU？
   * 操作系统必须以高性能的方式虚拟化CPU，同时保持对系统的控制。为此，需要硬件和操作系统支持。操作系统通常会明智地利用硬件支持，以便高效地实现其工作。

## 基本技巧：受限直接执行
![direct](./pictures/direct.png)
上表是无限制的直接执行协议，有如下两个问题：
* 如果我们只运行一个程序，操作系统怎么能确保程序不做任何我们不希望它做的事情，同时仍然高效地运行它？
* 当我们运行一个进程时，操作系统如何让它停下来并切换到另一个进程，从而实现虚拟化CPU所需的时分共享？

### 关键问题1：受限制的操作
* 如何执行受限制的操作？
   * 一个进程必须能够执行I/O和其他一些受限制的操作，但又不能让进程完全控制系统。操作系统和硬件如何写作实现这一点？

* 操作系统通过在用户模式(user mode)和内核模式(kernel mode)之间进行切换，使进程能受到一定的限制。用户如果想执行某种特权模式操作，可
   * 通过系统调用，使进程执行特殊的陷阱(trap)指令，同时跳入内核并将特权级别提升到内核模式
   * 进入内核模式后，通过陷阱表(trap table)，执行需要的特权操作
   * 完成后，操作系统调用一个特殊的从陷阱返回(return-from-trap)指令，返回到用户模式

* Limited Direct Execution (LDE)
LDE协议有两个阶段：
   * 第一个阶段在系统引导时，内核初始化陷阱表，并且CPU记住它的位置以供随后使用
   * 第二阶段在运行进程时，引导程序在使用从陷阱返回指令开始执行进程之前，会为进程做一些初始化工作(如，内存分配)

![LED](./pictures/LDE.png)
上表展示了内核模式和用户模式的切换过程，注意这和函数调用栈的区别。普通函数调用只涉及用户栈，不涉及内核栈。
当从内核模式转向用户模式时，硬件隐式地从内核栈中恢复寄存器；当通过系统调用从用户模式转向内核模式时，也是硬件隐式地将寄存器保存在内核栈，再跳转到内核模式。

### 关键问题2：在进程之间切换
* 如何重获CPU的控制权？
   * 操作系统如何重新获得CPU的控制权(regain control)，以便它可以在进程之间切换？

* 协作方式：等待系统调用
   * 在协作调度系统中，OS通过等待系统调用，或某种非法操作发生，从而重新获得CPU的控制权。进而完成进程切换，或终止违规进程。

* 非协作方式：操作系统进行控制
   * 利用时钟中断(timer interrupt)，在即使进程不协作的情况下，操作系统也可以获得CPU的控制权
   * 产生中断时，当前正在运行的进程停止，操作系统中预先配置的中断处理程序(interrupt handler)会运行

### 关键问题3：保存和恢复上下文
* 当操作系统通过协作或非协作方式获得控制权后，如何决定时继续运行当前进程，还是切换到另一个进程？
   * 由调度程序(scheduler)决定
      * 如果决定进行进程切换，OS就会执行上下文切换(context switch)
         * 为当前正在执行的进程保存一些寄存器(到被切换进程的内核栈)
         * 为即将执行的进程恢复一些寄存器的值(从要切换进程的内核栈)
         * 执行陷阱返回指令后，不是返回老进程，而是执行新进程

![process_switch](./pictures/process_switch.png)
上表中有两种类型的寄存器保存/恢复。
* 第一种是发生时钟中断的时候，用户寄存器由硬件隐式保存到内核栈
* 第二种是当操作系统从A切换到B，内核寄存器被OS明确地保存，但这次被存储在该进程的进程结构的内存中。而后，从另一进程结构中恢复寄存器，好像刚刚由B陷入内核而不是A陷入内核一样。

# 第7章 进程调度：介绍
## 关键问题
* 如何开发调度策略？
   * 我们该如何开发一个考虑调度策略的基本框架？什么时关键假设？哪些指标非常重要？哪些基本方法已经在早期的系统中使用过？

## 调度指标
* 非分时系统
   * T(周转时间) = T(完成时间) - T(到达时间)
* 分时系统
   * T(响应时间) = T(首次运行) - T(到达时间)

## 调度策略对比
* 先进先出(FIFO)
   * 问题：如果第一个任务时间远大于后面的任务，T(周转时间)会远大于后面的任务先执行，违背了SJF原则
* 最短任务优先(SJF, Shortest Job First)
   * 问题：如果执行时间少的任务晚于执行时间大的任务达到，在不抢占正在执行任务的前提下，仍然存在问题
* 最短完成时间优先(STCF)
   * 每当新工作进入系统时，确定剩余工作和新工作中谁的剩余时间最少，然后调度该工作
* 轮转(RR, Round Robin)
   * RR在一个时间片(time slice)内运行一个工作，然后切换到运行队列中的下一个任务
   * 如果周转时间是我们的指标，那么RR确实是最糟糕的策略之一。因为周转时间只关心作业何时完成。

## 结合I/O
![schedule_io](./pictures/schedule_io.png)

# 第8章 调度：多级反馈队列 Multi-Level Feedback Queue, MLFQ
多级反馈队列解决了：
* 在不知道工作要运行多久时，无法使用SJF或STCF优化周转时间
* 优化响应时间，但不能像轮转那样牺牲了周转时间

## 关键问题
* 没有完备的知识如何调度？
   * 没有工作长度的先验知识，如何设计一个能同时减少响应时间和周转时间的调度程序？

## MLFQ 基本规则
MLFQ中有许多独立的队列，每个队列有不同的优先级。任何时刻，一个工作只能存在于一个队列中。MLFQ总是优先执行较高优先级的工作。每个队列中可能会有多个工作，因此具有同样的优先级。在这种情况下，我们就对这些工作采用轮转调度。
* 如何设置优先级？
   * 根据观察到的行为，动态调整每个工作的优先级
      * 如果一个工作不断放弃CPU去等待键盘输入，这是交互型进程的可能行为，MLFQ因此会让它保持高优先级
      * 如果一个工作长时间地占用CPU，MLFQ会降低其优先级
* 规则
   * 如果A的优先级 > B的优先级，运行A(不运行B)
   * 如果A的优先级 = B的优先级，轮转运行A和B

## 尝试1：如何改变优先级
* 规则
   * 工作进入系统时，放在最高优先级
   * 工作用完整个时间片后，降低其优先级
   * 如果工作在其时间片以内主动释放CPU，则优先级不变
* 实例1：单个长工作
   * 对于单个长工作来说，最终任务会一直在低优先级的队列中运行
* 实例2：来了一个短工作
   * 如果来了一个短工作，由于它的运行时间很短，在被移入最低优先级队列之前就执行完了
   * 目标：如果不知道工作是短工作还是长工作，那么就在开始的时候假设其是短工作，并赋予最高优先级。
* 实例3：如果有I/O呢
   * 如果进程在时间片用完之前主动放弃CPU，则保持它的优先级不变
* 总结
   * 优点
      * 长工作之间可以公平地分享CPU
      * 又能给短工作或交互型工作很好的响应时间
   * 缺点
      * 会又饥饿问题，如果又太多交互型工作，长工作永远无法得到CPU

## 尝试2：提升优先级
* 规则
   * 经过一段时间S，就将系统中所有工作重新加入最高优先级队列

![schedule_tasks](./pictures/schedule_tasks.png)
* 问题
   * 如何设置S的值？
      * S设置得太高，长工作会饥饿
      * S设置得太低，交互型工作又得不到合适的CPU时间比例

## 尝试3： 更好的计时方式
* 规则
   * 为了防止调度程序被愚弄，一旦工作用完了其在某一层中的时间配额(无论中间主动放弃了多少次CPU)，就降低其优先级

## 小结
* MLFQ规则
   * 如果A的优先级 > B的优先级，运行A
   * 如果A的优先级 = B的优先级，轮转A和B
   * 工作进入系统时，放在最高优先级
   * 一旦工作用完了其在某一层中的时间配额(无论中间主动放弃了多少次CPU)，就降低其优先级
   * 经过一段时间S，就将系统中所有工作重新加入最高优先级队列

# 第9章 调度：比例份额
比例份额(proportional-share)调度，有时也称公平份额(fair-share)调度，目的是确保每个工作获得一定比例的CPU时间，而不是优化周转时间和响应时间。

## 彩票调度
比例份额调度有一个非常优秀的现代例子，就是彩票调度(lottery scheduling)，即利用随机性决定调度机制。随机性的好处有：
* 随机方法可以避免奇怪的边角情况
* 随机方法很轻量
* 随机方法很快

## 步长调度
步长调度可以解决彩票调度在短时长任务存在不公平的问题，但是需要全局状态。如一个新的进程在步长调度执行过程中加入系统，无法确定它的行程值。

## 比例份额调度的应用
由于以下的原因，比例份额调度并没有作为CPU调度程序被广泛使用：
* 不能很好地适合I/O
* 票数分配问题没有确定的解决方式，如：如何知道浏览器进程应该拥有多少票数？

比例份额调度只有在特定的领域得到应用。例如虚拟数据中心中，你可能会希望分配1/4的CPU周期给Windows虚拟机，剩余给Linux系统，比例分配的方式可以更简单高效。

# 第10章 多处理器调度(高级)
## 关键问题
* 多处理器系统面临的问题
   * 应用程序并行执行问题
      * 多线程应用可以将工作分散到多个CPU上，因此CPU资源越多就运行越快
   * 多处理器调度问题
      * 操作系统应该如何在多CPU上调度工作？会遇到什么新问题？已有的技术依旧适用吗？是否需要新的思路？

## 背景：多处理器架构
![multi_cpu](./pictures/multi_cpu.png)
### 缓存一致性(cache coherence)问题
多CPU由于存在多个缓存，会存在缓存一致性(cache coherence)问题。硬件提供了这个问题的基本解决方案：
* 通过监控内存访问，硬件可以保证获得正确的数据，并保证共享内存的唯一性。
   * 在基于总线的系统中，一种方式是使用总线窥探。每个缓存都通过监听链接所有缓存和内存的总线，来发现内存访问。如果CPU发现对它放在缓存中的数据的更新，会作废(invalidate)本地副本(从缓存中移除)，或更新(update)它。

### 缓存亲和度(cache affinity)问题
一个进程在某个CPU上运行时，会在该CPU的缓存中维护许多状态。下次该进程在相同CPU上运行时，由于缓存中的数据而执行得更快。相反，在不同的CPU上执行，会由于需要重新加载数据而很慢(好在硬件保证的缓存一致性可以保证正确执行)。因此多处理调度应该考虑到这种缓存亲和性，并尽可能将进程保持在同一个CPU上。

## 单队列调度 SQMS, Single Queue Multiprocessor Scheduling
简单地复用单处理器调度的基本架构，缺点是：
* 缺乏可扩展性scalability
   * 为了保证在多CPU上正常运行，调度程序的开发者需要在代码中通过加锁locking来保证原子性。在SQMS访问单个队列时(如寻找下一个运行的工作)，锁确保得到正确的结果。
* 缓存亲和性
   * 如：调度队列 A->B->C->D->E-NULL。下面是每个CPU可能的调度序列：<br>
   ![sqms](./pictures/sqms_1.png)
   * 即使做了如下改进，E任务还是没法保证缓存亲和性: <br>
   ![sqms](./pictures/sqms_1.png)

## 多队列调度 MQMS，Multi-Queue Multiprocessor Scheduling
在MQMS中，基本调度框架包含多个调度队列，每个队列可以使用不同的调度规则，比如轮转或其他任何可能的算法。当一个工作进入系统后，系统会依照一些启发性规则(如随机或选择较空的队列)将其放入某个调度队列。这样一来，每个CPU调度之间相互独立，就避免了单队列的方式中由于数据共享及同步带来的问题。
![mqms](./pictures/mqms.png)

### 关键问题
* 如何应对MQMS存在负载不均的问题
   * 多队列多处理器调度程序应该如何处理负载不均问题，从而更好地实现预期的调度目标？
* 通过迁移模式可以解决负载不均的问题，但是系统应该如何迁移？
   * 一个基本的方法是采用一种技术，名为工作窃取(working stealing)。通过这种方法，工作量较少的(源)队列不定期地"偷看"其他队列是不是比自己工作多。如果目标队列比源队列显著地更满，就从目标队列"窃取"一个或多个工作，实现负载均衡。

### Linux多处理器调度
Linux有三种调度模式：
* O(1)调度程序
   * 多队列
   * 基于优先级(类似MLFQ)
* 完全公平调度程序(CFS)
   * 多队列
   * 基于比例调度(类似步长调度)
* BF调度程序(BFS)
   * 单队列
   * 基于比例调度，最早最合适虚拟截止时间优先算法(EEVEF)

# 第13章 抽象：地址空间
## 关键问题
* 如何虚拟化内存
   * 操作系统如何在单一的物理内存上为多个运行的进程(所有进程共享内存)构建一个私有的、可能很大的地址空间的抽象？

## 虚拟内存系统的目标
* 透明transparency
   * 程序不应该感知到内存被虚拟化的事实
* 效率efficiency
   * 操作系统应该在时间和空间上更高效
* 保护protection
   * 保护进程不受其他进程影响

# 第14章 插叙：内存操作API
## 关键问题
* 如何分配和管理内存
   * 在UNIX/C程序中，理解如何分配和管理内存是构建健壮和可靠软件的重要基础。通常使用哪些接口？哪些错误需要避免？

## 内存类型
* 栈
   * 申请和释放操作时编译器来隐式管理的
* 堆
   * 申请和释放由人为控制

## 底层操作系统支持
`malloc`和`free`不是系统调用，而是库调用。它们底层是通过系统调用实现的。
### 系统调用brk
系统调用`brk`被用来改变程序分断的位置：堆结束的位置。它需要一个参数(新分断的地址)，从而根据新分断是大于还是小于当前分断，来增加或减小堆的大小。另一个调用`sbrk`要求传入一个增量，但目的是一样的。
### 系统调用mmap
通过`mmap`可以从操作系统获取内存。通过传入正确的参数，`mmap`可以在程序中创建一个匿名内存区域。这个区域不予任何特定文件相关联，而是于交换空间(swap space)相关联。

# 第15章 机制：地址转换
## 关键问题
* 如何高效、灵活地虚拟化内存
   * 如何实现高效的内存虚拟化？
   * 如何提供应用程序所需的灵活性？
   * 如何保持控制应用程序可访问的内存位置，从而确保应用程序的内存访问受到合理的限制？
   * 如何高效得实现这一切？

## 基于硬件的地址转换(hardware-based address translation)
硬件对每次内存访问进行处理，将指令中的虚拟地址转换为数据实际存储的物理地址。当然，仅仅依靠硬件不足以实现虚拟内存，因为它只是提供了底层机制来提高效率。操作系统必须在关键的位置介入，设置好硬件，以便完成正确的地址转换。因此它必须管理内存(manage memory)，记录被占用和空闲的内存位置，并明智而谨慎地介入，保持对内存使用的控制。

## 一个例子
设想一个进程的地址空间如下图所示：
![simple_mem](./pictures/simple_mem.png)
其中，代码段C语言形式如下：
```c
void func() {
   int x;
   x = x + 3;
...
```
汇编形式如下：
```asm
movl 0x0(%ebx), %eax ;load 0+ebx into eax
addl $0x03, %eax     ;add 3 to eax register
movl %eax, 0x0(%ebx) ;store eax back to mem
```
上面的汇编代码假定x的地址已经存入寄存器ebx，之后通过movl指令将这个地址的值加载到通用寄存器eax。吓一跳指令对eax的内容加3。最后一条指令将eax中的值协会到内存的同一位置。

在上图中，代码和数据都位于进程的地址空间，3条指令序列位于地址128(靠近头部的代码段)，变量x的值位于地址15KB(在靠近底部的栈中)，x的初始值是3000。

如果这3条指令执行，从进程的角度来看，发生了以下几次内存访问：
* 从地址128获取指令；
* 执行指令(从地址15KB加载数据)；
* 从地址132获取指令；
* 执行指令(没有内存访问)；
* 从地址135获取指令；
* 执行指令(新值存入地址15KB)。

从程序的角度看，它的地址空间从0开始到16KB结束。操作系统希望将这个进程地址空间放在物理内存的其他位置，有如下几个问题：
* 怎样在内存中重定位这个进程，同时对该进程透明？
* 怎么样提供一种虚拟地址空间从0开始的假象，而实际上地址空间位于另外某个物理地址？

下图展示了一个例子，说明这个进程的地址空间被放入物理内存后可能的样子。操作系统将第一块物理内存留给了自己，并将上述例子中的进程地址空间重定位到从32KB开始的物理内存地址。剩下的两块内存空闲(16~32KB和48~64KB)。
![simple_mem_relocate.png](./pictures/simple_mem_relocate.png)

## 动态(基于硬件)重定位
基址加界限机制(base and bound)，有时又称为动态重定位(dynamic relocation)。具体来说，每个CPU需要两个硬件寄存器：
* 基址(base)寄存器
* 界限(bound)寄存器

这组基址和界限寄存器，让我们能够将地址空间放在物理内存的任何位置，同时又能确保进程只能访问自己的地址空间。进程中使用的内存引用都是虚拟地址，硬件接下来将虚拟地址加上基址寄存器中的内容，得到物理地址，再发给内存系统。

* 基于软件的静态重定位
   * 一些早期系统采用纯软件的重定位方式，称为静态重定位
   * 存在访问保护问题，而且很难将内存空间重定位到其他位置
* 基于硬件的动态重定位
   * 一个基址寄存器将虚拟地址转换为物理地址
   * 一个界限寄存器确保这个地址在进程空间的范围内

这种基址寄存器配合界限寄存器的硬件结构是存在芯片中的，如果进程需要访问超过这个界限或者为负数的虚拟地址，CPU将触发异常。有时我们将CPU的这个负载地址转换的部分统称为内存管理单元(Memory Management Unit, MMU)。

## 重定位硬件支持：总结
![hardware_require](./pictures/hardware_require.png)

## 操作系统的职责
![os_job](./pictures/os_job.png)

## 动态重定位的受限执行协议
![relocation](./pictures/relocation.png)

## 小结
简单的动态重定位技术又效率低下的问题。例如，上面重定位的进程使用了从32KB到48KB的物理内存，但由于该进程的栈区和堆区并不大，导致这块内存区域中大量的空间被浪费。这种浪费通常称为内部碎片(internal fragmentation)，指的是已经分配的内存单元内部有未使用的空间(即碎片)。第一次尝试解决是将基址加界限的概念稍稍泛化，得到分段(segmentation)的概念。

# 第16章 分段
## 关键问题
* 怎么支持大地址空间
   * 怎么支持大地址空间，高效利用栈和堆之间可能出现的大量空闲空间？

## 分段：泛化的基址/界限
在MMU中引入不止一个基址和界限寄存器对，而是给地址空间内的每个逻辑段(segment)一对。一个段只是地址空间里的一个连续定长的区域。在典型的地址空间里有3个逻辑不同的段：代码、栈和堆。
![mmu_seg](./pictures/mmu_seg.png)
为了达到上表的映射关系，需要3对基址和界限寄存器。

## 我们引用哪个段
### 关键问题
* 硬件在地址转换时使用段寄存器。它如何知道段内的偏移量，以及地址引用了哪个段?

### 显式(explicit)方式
显示方式就是用虚拟地址的开头几位来标识不同的段。例如下图的虚拟地址，前两位(01)告诉硬件我们引用哪个段。剩下的12位时段内偏移。偏移量与基址寄存器相加，硬件就得到了最终的物理地址。
![explicit_seg](./pictures/explicit_seg.png)

### 隐式(implicit)方式
硬件通过地址产生的方式来确定段。例如，如果地址由程序计数器产生(即它是指令获取)，那么地址在代码段。如果基于栈或基址指针，它一定在栈段。其他地址则在堆段。

## 支持共享
随着分段机制的不断改进，系统设计人员很快意识到，通过再多一点的硬件支持，就能实现新的效率提升。具体来说，要节省内存，有时候再地址空间之间共享(share)某些内存段是有用的。为了支持共享，硬件需要保护位来标识程序是否能读写该段，或执行其中的代码。

## 细粒度与粗粒度的分段
支持更加细粒度的段，需要在内存中保存某种段表(segment table)。有了操作系统和硬件的支持，编译器可以将代码段和数据段划分位许多不同的部分，从而可以更高效地利用内存。

## 操作系统支持
分段带来的新问题：
* 操作系统上下文切换时应该做什么？
   * 各个段寄存器中的内容必须保存和恢复。
* 如何管理物理内存的空闲空间？
   * 因为每个段的大小可能不同，物理内存会充满许多空闲空间的小洞，因而很难分配给新的段，或扩大已有的段。这种问题被称为外部碎片。

### 外部碎片问题解决方案
* 紧凑物理内存
   * 目的：重新安排原有的段
   * 操作系统先终止运行的进程，规整到连续的内存中，再重新运行
   * 缺点是成本很高，因为拷贝段时内存密集型，一般会占用大量的处理时间
   ![mem_seg](./pictures/mem_seg.png)
* 利用空闲列表管理法
   * 目的：试图保留大的内存块用于分配
   * 实现：存在很多算法
      * 最优匹配
      * 最坏匹配
      * 首次匹配
      * 伙伴算法

## 小结
分段的优点：
* 避免地址空间的逻辑段之间的大量潜在的内存浪费
* 更好支持稀疏地址空间
* 适合硬件实现，开销很小
* 可代码共享

分段的缺点
* 由于段的大小不同，引入了外部碎片
* 分段还不足以支持更一般化的系数地址空间。例如，如果有一个很大但是稀疏的堆，都在一个逻辑段中，整个堆仍然必须完整地加载到内存中

# 第17章 空闲空间管理
* 如果需要管理的空间被划分为固定大小的单元，管理空闲空间就很容易。在这种情况下，只需要维护这些大小固定的单元的列表，如果由请求，就返回列表中的第一项。
* 如果要管理的空间由大小不同的单元构成，管理就变得困难。这种情况会出现在用户级的内存分配库(如malloc()和free())，或者操作系统用分段(segmentation)的方式实现虚拟内存。这两种情况下，出现了外部碎片的问题：
   * 空闲空间被分割成不同大小的小块，成为碎片，后续的请求可能失败，因为没有一块足够大的连续空闲空间，即使这是总的空闲空间超出了请求的大小。

## 关键问题
* 如何管理空闲空间
   * 要满足变长的分配请求，应该如何管理空闲空间？
   * 什么策略可以让碎片最小化？
   * 不同方法的时间和空间开销如何？

## 底层机制
### 分割与合并
空闲列表包含一组元素，记录了堆中的哪些空间还没有分配。
![space_list](./pictures/space_list.png)
* 分割
此时，任何大于10字节的分配请求都会失败。如果只申请一个字节的内存，分配程序会执行所谓的分割(splitting)动作，空闲列表会边变成这样：
![space_list2](./pictures/space_list2.png)
* 合并
如果通过free释放掉某个空间时，会检测其前后是否时空闲的，如果是，则合并到一块。大多数分配程序都会在头块中保存一点额外的信息，记录分配块的大小。因此实际分配或释放的空间要大于用户指定的大小。

## 嵌入空闲列表
下面介绍内存分配的过程，其中一个列表节点描述：
```c
typedef struct node_t {
   int size;
   struct node_t* next;
} node_t;
```
构建4096大小的堆：
```c
// mmap() returns a pointer to a chunk of free space
node_t* head = mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_ANON|MAP_PRIVATE, -1, 0);
head->size = 4096 - sizeof(node_t);
head->next = NULL;
```
![malloc1](./pictures/malloc1.png)
![malloc2](./pictures/malloc2.png)
最后一幅图的空闲列表包括一个小空闲块(100字节，由列表的头指向)和一个大空闲块(3764字节)。现在假设剩余的两块已分配的空间也被释放，没有合并，空闲列表将非常破碎，如下图：
![malloc3](./pictures/malloc3.png)
解决方案很简单：遍历列表，合并相邻块。

## 让堆增长
大多数传统的分配程序会从很小的堆开始，当空间耗尽时，再向操作系统申请更大的空间。通常，这意味着它们进行了某种系统调用(如UNIX系统中的sbrk)，让堆增长。操作系统在执行sbrk系统调用时，会找到空闲的物理内存页，将它们映射到请求进程的地址空间中去，并返回新的堆的末尾地址，并返回新的堆的末尾地址。如果内存空间耗尽，则返回NULL表示失败。

## 基本策略
### 最优匹配
首先遍历整个空闲列表，找到和请求大小一样或更大的空闲块，然后返回这组候选者中最小的一块。
* 优点
   * 避免空间浪费
* 缺点
   * 遍历查找开销大

### 最差匹配
与最优匹配相反，尝试找最大的空闲块，分割并满足用户需求后，将剩余的块加入空闲列表。大多数研究表明它的表现非常差，导致过量的碎片，同时还有很大开销。

### 首次匹配
找到第一个足够大的块，将请求的空间返回给用户。同样，剩余的空闲空间留给后续请求。
* 优点
   * 不需要遍历所有空闲块
* 缺点
   * 会让空闲列表开头的部分由很多小块

### 下次匹配
多维护一个指针，指向上次查找结束的位置。其想法时将堆空闲空间的查找操作扩散到整个列表中去，避免堆列表开头频繁的分割。
* 优点
   * 性能和首次匹配相近，同样避免了遍历查找

### 分离空闲列表
如果某个应用程序经常申请一种或几种大小的内存空间，那就用一个独立的列表，只管理这样大小的对象。其他大小的请求都交给更通用的内存分配程序。
* 优点
   * 针对经常申请的内存空间，不存在碎片问题
   * 没有复杂的列表查找过程
* 应该拿出多少内存来专门为某种大小的请求服务？
   * 内核启动时，对频繁申请的对象，为其创建特定的空闲列表
   * 如果空闲空间快耗尽时，就向通用内存分配程序申请一些内存厚块slab
   * 如果给定厚块中对象的引用计数变为0，通用的内存分配程序可以从专门的分配程序中回收这些空间
   * 厚块分配程序避免了频繁的初始化和销毁，从而显著降低了开销

### 伙伴系统
针对合并的开销，二分伙伴分配程序让合并变得简单。
![allocator](./pictures/buddy_allocator.png)
* 空闲空间从概念上被看成大小为2^N的大空间
* 如果用户请求7KB空间，会按上图分配8KB空间给用户，因此存在内部碎片
* 如果这8KB空间释放时，此算法会不断上溯合并。如8KB和8KB合并为16KB，32KB...
